{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Keras model on all features </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.preprocessing import StandardScaler # Used for scaling of data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_CWD</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_Con</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196.0</td>\n",
       "      <td>706</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162.0</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350.0</td>\n",
       "      <td>655</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1456</td>\n",
       "      <td>60</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1999</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1457</td>\n",
       "      <td>20</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1978</td>\n",
       "      <td>1988</td>\n",
       "      <td>119.0</td>\n",
       "      <td>790</td>\n",
       "      <td>163</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1458</td>\n",
       "      <td>70</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1941</td>\n",
       "      <td>2006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>275</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1459</td>\n",
       "      <td>20</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1950</td>\n",
       "      <td>1996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49</td>\n",
       "      <td>1029</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>20</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1965</td>\n",
       "      <td>1965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>830</td>\n",
       "      <td>290</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows Ã— 304 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "Id                                                                            \n",
       "1             60         65.0     8450            7            5       2003   \n",
       "2             20         80.0     9600            6            8       1976   \n",
       "3             60         68.0    11250            7            5       2001   \n",
       "4             70         60.0     9550            7            5       1915   \n",
       "5             60         84.0    14260            8            5       2000   \n",
       "...          ...          ...      ...          ...          ...        ...   \n",
       "1456          60         62.0     7917            6            5       1999   \n",
       "1457          20         85.0    13175            6            6       1978   \n",
       "1458          70         66.0     9042            7            9       1941   \n",
       "1459          20         68.0     9717            5            6       1950   \n",
       "1460          20         75.0     9937            5            6       1965   \n",
       "\n",
       "      YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  SaleType_CWD  \\\n",
       "Id                                                      ...                 \n",
       "1             2003       196.0         706           0  ...             0   \n",
       "2             1976         0.0         978           0  ...             0   \n",
       "3             2002       162.0         486           0  ...             0   \n",
       "4             1970         0.0         216           0  ...             0   \n",
       "5             2000       350.0         655           0  ...             0   \n",
       "...            ...         ...         ...         ...  ...           ...   \n",
       "1456          2000         0.0           0           0  ...             0   \n",
       "1457          1988       119.0         790         163  ...             0   \n",
       "1458          2006         0.0         275           0  ...             0   \n",
       "1459          1996         0.0          49        1029  ...             0   \n",
       "1460          1965         0.0         830         290  ...             0   \n",
       "\n",
       "      SaleType_ConLw  SaleType_Con  SaleType_Oth  SaleCondition_Normal  \\\n",
       "Id                                                                       \n",
       "1                  0             0             0                     1   \n",
       "2                  0             0             0                     1   \n",
       "3                  0             0             0                     1   \n",
       "4                  0             0             0                     0   \n",
       "5                  0             0             0                     1   \n",
       "...              ...           ...           ...                   ...   \n",
       "1456               0             0             0                     1   \n",
       "1457               0             0             0                     1   \n",
       "1458               0             0             0                     1   \n",
       "1459               0             0             0                     1   \n",
       "1460               0             0             0                     1   \n",
       "\n",
       "      SaleCondition_Abnorml  SaleCondition_Partial  SaleCondition_AdjLand  \\\n",
       "Id                                                                          \n",
       "1                         0                      0                      0   \n",
       "2                         0                      0                      0   \n",
       "3                         0                      0                      0   \n",
       "4                         1                      0                      0   \n",
       "5                         0                      0                      0   \n",
       "...                     ...                    ...                    ...   \n",
       "1456                      0                      0                      0   \n",
       "1457                      0                      0                      0   \n",
       "1458                      0                      0                      0   \n",
       "1459                      0                      0                      0   \n",
       "1460                      0                      0                      0   \n",
       "\n",
       "      SaleCondition_Alloca  SaleCondition_Family  \n",
       "Id                                                \n",
       "1                        0                     0  \n",
       "2                        0                     0  \n",
       "3                        0                     0  \n",
       "4                        0                     0  \n",
       "5                        0                     0  \n",
       "...                    ...                   ...  \n",
       "1456                     0                     0  \n",
       "1457                     0                     0  \n",
       "1458                     0                     0  \n",
       "1459                     0                     0  \n",
       "1460                     0                     0  \n",
       "\n",
       "[1460 rows x 304 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in train data\n",
    "trainfull = pd.read_csv('trainfull_ohe.csv', index_col=0)\n",
    "trainfull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_ohe.csv', index_col=0)\n",
    "val = pd.read_csv('val_ohe.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.fillna(0)\n",
    "val = val.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create inputs and output for train and val set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_train = list(train.columns)\n",
    "col_train.remove('SalePrice')\n",
    "Features = col_train\n",
    "X_train = train[Features]\n",
    "X_val = val[Features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['SalePrice'].values\n",
    "y_val = val['SalePrice'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tf2/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/opt/conda/envs/tf2/lib/python3.7/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "scale = StandardScaler()\n",
    "X_train = scale.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tf2/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/opt/conda/envs/tf2/lib/python3.7/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "scale = StandardScaler()\n",
    "X_val = scale.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create, compile and fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1168 samples, validate on 292 samples\n",
      "Epoch 1/150\n",
      "1168/1168 [==============================] - 1s 607us/step - loss: 181441.5423 - val_loss: 178839.8106\n",
      "Epoch 2/150\n",
      "1168/1168 [==============================] - 1s 448us/step - loss: 181441.5419 - val_loss: 178839.8106\n",
      "Epoch 3/150\n",
      "1168/1168 [==============================] - 1s 449us/step - loss: 181441.5416 - val_loss: 178839.8105\n",
      "Epoch 4/150\n",
      "1168/1168 [==============================] - 1s 448us/step - loss: 181441.5415 - val_loss: 178839.8105\n",
      "Epoch 5/150\n",
      "1168/1168 [==============================] - 1s 497us/step - loss: 181441.5404 - val_loss: 178839.8105\n",
      "Epoch 6/150\n",
      "1168/1168 [==============================] - 1s 445us/step - loss: 181441.5387 - val_loss: 178839.8094\n",
      "Epoch 7/150\n",
      "1168/1168 [==============================] - 1s 448us/step - loss: 181441.5368 - val_loss: 178839.8057\n",
      "Epoch 8/150\n",
      "1168/1168 [==============================] - 1s 442us/step - loss: 181441.5321 - val_loss: 178839.8014\n",
      "Epoch 9/150\n",
      "1168/1168 [==============================] - 1s 460us/step - loss: 181441.5271 - val_loss: 178839.7966\n",
      "Epoch 10/150\n",
      "1168/1168 [==============================] - 1s 463us/step - loss: 181441.5226 - val_loss: 178839.7901\n",
      "Epoch 11/150\n",
      "1168/1168 [==============================] - 1s 480us/step - loss: 181441.5169 - val_loss: 178839.7880\n",
      "Epoch 12/150\n",
      "1168/1168 [==============================] - 1s 441us/step - loss: 181441.5156 - val_loss: 178839.7847\n",
      "Epoch 13/150\n",
      "1168/1168 [==============================] - 1s 455us/step - loss: 181441.5142 - val_loss: 178839.7842\n",
      "Epoch 14/150\n",
      "1168/1168 [==============================] - 1s 451us/step - loss: 181441.5133 - val_loss: 178839.7831\n",
      "Epoch 15/150\n",
      "1168/1168 [==============================] - 1s 453us/step - loss: 181441.5122 - val_loss: 178839.7820\n",
      "Epoch 16/150\n",
      "1168/1168 [==============================] - 1s 464us/step - loss: 181441.5105 - val_loss: 178839.7820\n",
      "Epoch 17/150\n",
      "1168/1168 [==============================] - 1s 471us/step - loss: 181441.5112 - val_loss: 178839.7815\n",
      "Epoch 18/150\n",
      "1168/1168 [==============================] - 1s 460us/step - loss: 181441.5092 - val_loss: 178839.7799\n",
      "Epoch 19/150\n",
      "1168/1168 [==============================] - 1s 464us/step - loss: 181441.5073 - val_loss: 178839.7771\n",
      "Epoch 20/150\n",
      "1168/1168 [==============================] - 1s 462us/step - loss: 181441.5037 - val_loss: 178839.7718\n",
      "Epoch 21/150\n",
      "1168/1168 [==============================] - 1s 466us/step - loss: 181441.5017 - val_loss: 178839.7669\n",
      "Epoch 22/150\n",
      "1168/1168 [==============================] - 1s 461us/step - loss: 181441.4958 - val_loss: 178839.7616\n",
      "Epoch 23/150\n",
      "1168/1168 [==============================] - 1s 448us/step - loss: 181441.4891 - val_loss: 178839.7589\n",
      "Epoch 24/150\n",
      "1168/1168 [==============================] - 1s 464us/step - loss: 181441.4842 - val_loss: 178839.7529\n",
      "Epoch 25/150\n",
      "1168/1168 [==============================] - 1s 446us/step - loss: 181441.4802 - val_loss: 178839.7486\n",
      "Epoch 26/150\n",
      "1168/1168 [==============================] - 1s 453us/step - loss: 181441.4743 - val_loss: 178839.7464\n",
      "Epoch 27/150\n",
      "1168/1168 [==============================] - 1s 458us/step - loss: 181441.4725 - val_loss: 178839.7411\n",
      "Epoch 28/150\n",
      "1168/1168 [==============================] - 1s 467us/step - loss: 181441.4664 - val_loss: 178839.7362\n",
      "Epoch 29/150\n",
      "1168/1168 [==============================] - 1s 447us/step - loss: 181441.4608 - val_loss: 178839.7287\n",
      "Epoch 30/150\n",
      "1168/1168 [==============================] - 1s 464us/step - loss: 181441.4534 - val_loss: 178839.7216\n",
      "Epoch 31/150\n",
      "1168/1168 [==============================] - 1s 461us/step - loss: 181441.4446 - val_loss: 178839.7157\n",
      "Epoch 32/150\n",
      "1168/1168 [==============================] - 1s 464us/step - loss: 181441.4402 - val_loss: 178839.7082\n",
      "Epoch 33/150\n",
      "1168/1168 [==============================] - 1s 462us/step - loss: 181441.4319 - val_loss: 178839.7039\n",
      "Epoch 34/150\n",
      "1168/1168 [==============================] - 1s 454us/step - loss: 181441.4235 - val_loss: 178839.6888\n",
      "Epoch 35/150\n",
      "1168/1168 [==============================] - 1s 448us/step - loss: 181441.4126 - val_loss: 178839.6796\n",
      "Epoch 36/150\n",
      "1168/1168 [==============================] - 1s 453us/step - loss: 181441.4011 - val_loss: 178839.6726\n",
      "Epoch 37/150\n",
      "1168/1168 [==============================] - 1s 468us/step - loss: 181441.3891 - val_loss: 178839.6607\n",
      "Epoch 38/150\n",
      "1168/1168 [==============================] - 1s 476us/step - loss: 181441.3791 - val_loss: 178839.6489\n",
      "Epoch 39/150\n",
      "1168/1168 [==============================] - 1s 515us/step - loss: 181441.3642 - val_loss: 178839.6348\n",
      "Epoch 40/150\n",
      "1168/1168 [==============================] - 1s 488us/step - loss: 181441.3504 - val_loss: 178839.6192\n",
      "Epoch 41/150\n",
      "1168/1168 [==============================] - 1s 467us/step - loss: 181441.3325 - val_loss: 178839.6009\n",
      "Epoch 42/150\n",
      "1168/1168 [==============================] - 1s 466us/step - loss: 181441.3153 - val_loss: 178839.5826\n",
      "Epoch 43/150\n",
      "1168/1168 [==============================] - 1s 449us/step - loss: 181441.2925 - val_loss: 178839.5605\n",
      "Epoch 44/150\n",
      "1168/1168 [==============================] - 1s 453us/step - loss: 181441.2673 - val_loss: 178839.5384\n",
      "Epoch 45/150\n",
      "1168/1168 [==============================] - 1s 465us/step - loss: 181441.2434 - val_loss: 178839.5087\n",
      "Epoch 46/150\n",
      "1168/1168 [==============================] - 1s 449us/step - loss: 181441.2122 - val_loss: 178839.4786\n",
      "Epoch 47/150\n",
      "1168/1168 [==============================] - 1s 471us/step - loss: 181441.1797 - val_loss: 178839.4456\n",
      "Epoch 48/150\n",
      "1168/1168 [==============================] - 1s 468us/step - loss: 181441.1417 - val_loss: 178839.4052\n",
      "Epoch 49/150\n",
      "1168/1168 [==============================] - 1s 490us/step - loss: 181441.0972 - val_loss: 178839.3584\n",
      "Epoch 50/150\n",
      "1168/1168 [==============================] - 1s 443us/step - loss: 181441.0475 - val_loss: 178839.3125\n",
      "Epoch 51/150\n",
      "1168/1168 [==============================] - 1s 464us/step - loss: 181440.9904 - val_loss: 178839.2505\n",
      "Epoch 52/150\n",
      "1168/1168 [==============================] - 1s 470us/step - loss: 181440.9248 - val_loss: 178839.1836\n",
      "Epoch 53/150\n",
      "1168/1168 [==============================] - 1s 462us/step - loss: 181440.8480 - val_loss: 178839.1028\n",
      "Epoch 54/150\n",
      "1168/1168 [==============================] - 1s 454us/step - loss: 181440.7576 - val_loss: 178839.0058\n",
      "Epoch 55/150\n",
      "1168/1168 [==============================] - 1s 464us/step - loss: 181440.6531 - val_loss: 178838.8959\n",
      "Epoch 56/150\n",
      "1168/1168 [==============================] - 1s 461us/step - loss: 181440.5258 - val_loss: 178838.7638\n",
      "Epoch 57/150\n",
      "1168/1168 [==============================] - 1s 467us/step - loss: 181440.3742 - val_loss: 178838.6042\n",
      "Epoch 58/150\n",
      "1168/1168 [==============================] - 1s 455us/step - loss: 181440.1934 - val_loss: 178838.4097\n",
      "Epoch 59/150\n",
      "1168/1168 [==============================] - 1s 460us/step - loss: 181439.9693 - val_loss: 178838.1730\n",
      "Epoch 60/150\n",
      "1168/1168 [==============================] - 1s 445us/step - loss: 181439.6957 - val_loss: 178837.8814\n",
      "Epoch 61/150\n",
      "1168/1168 [==============================] - 1s 452us/step - loss: 181439.3552 - val_loss: 178837.5106\n",
      "Epoch 62/150\n",
      "1168/1168 [==============================] - 1s 461us/step - loss: 181438.9243 - val_loss: 178837.0384\n",
      "Epoch 63/150\n",
      "1168/1168 [==============================] - 1s 463us/step - loss: 181438.3655 - val_loss: 178836.4266\n",
      "Epoch 64/150\n",
      "1168/1168 [==============================] - 1s 460us/step - loss: 181437.6329 - val_loss: 178835.6148\n",
      "Epoch 65/150\n",
      "1168/1168 [==============================] - 1s 453us/step - loss: 181436.6503 - val_loss: 178834.5109\n",
      "Epoch 66/150\n",
      "1168/1168 [==============================] - 1s 460us/step - loss: 181435.2946 - val_loss: 178832.9575\n",
      "Epoch 67/150\n",
      "1168/1168 [==============================] - 1s 459us/step - loss: 181433.3472 - val_loss: 178830.6873\n",
      "Epoch 68/150\n",
      "1168/1168 [==============================] - 1s 471us/step - loss: 181430.4309 - val_loss: 178827.2032\n",
      "Epoch 69/150\n",
      "1168/1168 [==============================] - 1s 467us/step - loss: 181425.8113 - val_loss: 178821.5006\n",
      "Epoch 70/150\n",
      "1168/1168 [==============================] - 1s 539us/step - loss: 181417.8959 - val_loss: 178811.2875\n",
      "Epoch 71/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 1s 462us/step - loss: 181402.7334 - val_loss: 178790.3949\n",
      "Epoch 72/150\n",
      "1168/1168 [==============================] - 1s 488us/step - loss: 181368.0669 - val_loss: 178737.3561\n",
      "Epoch 73/150\n",
      "1168/1168 [==============================] - 1s 483us/step - loss: 181258.2372 - val_loss: 178531.1390\n",
      "Epoch 74/150\n",
      "1168/1168 [==============================] - 1s 490us/step - loss: 180381.5735 - val_loss: 175394.8389\n",
      "Epoch 75/150\n",
      "1168/1168 [==============================] - 1s 462us/step - loss: 83288.2481 - val_loss: 28121.9138\n",
      "Epoch 76/150\n",
      "1168/1168 [==============================] - 1s 444us/step - loss: 25886.5102 - val_loss: 24397.8603\n",
      "Epoch 77/150\n",
      "1168/1168 [==============================] - 1s 453us/step - loss: 21244.4009 - val_loss: 29992.5068\n",
      "Epoch 78/150\n",
      "1168/1168 [==============================] - 1s 442us/step - loss: 19531.2923 - val_loss: 21807.2239\n",
      "Epoch 79/150\n",
      "1168/1168 [==============================] - 1s 458us/step - loss: 17851.7514 - val_loss: 20636.2416\n",
      "Epoch 80/150\n",
      "1168/1168 [==============================] - 1s 439us/step - loss: 17149.6759 - val_loss: 20590.4181\n",
      "Epoch 81/150\n",
      "1168/1168 [==============================] - 1s 458us/step - loss: 15999.0814 - val_loss: 19084.9904\n",
      "Epoch 82/150\n",
      "1168/1168 [==============================] - 1s 443us/step - loss: 15421.7284 - val_loss: 18948.7233\n",
      "Epoch 83/150\n",
      "1168/1168 [==============================] - 1s 461us/step - loss: 15003.0943 - val_loss: 19814.7514\n",
      "Epoch 84/150\n",
      "1168/1168 [==============================] - 1s 442us/step - loss: 14758.7857 - val_loss: 19449.7095\n",
      "Epoch 85/150\n",
      "1168/1168 [==============================] - 1s 456us/step - loss: 14629.0442 - val_loss: 18476.5627\n",
      "Epoch 86/150\n",
      "1168/1168 [==============================] - 1s 443us/step - loss: 14243.6494 - val_loss: 18483.8485\n",
      "Epoch 87/150\n",
      "1168/1168 [==============================] - 1s 450us/step - loss: 14129.7260 - val_loss: 18458.2915\n",
      "Epoch 88/150\n",
      "1168/1168 [==============================] - 1s 440us/step - loss: 14021.9941 - val_loss: 18422.3751\n",
      "Epoch 89/150\n",
      "1168/1168 [==============================] - 1s 455us/step - loss: 13911.5328 - val_loss: 18627.8873\n",
      "Epoch 90/150\n",
      "1168/1168 [==============================] - 1s 440us/step - loss: 13820.7364 - val_loss: 18527.4634\n",
      "Epoch 91/150\n",
      "1168/1168 [==============================] - 1s 441us/step - loss: 13780.0487 - val_loss: 18298.9293\n",
      "Epoch 92/150\n",
      "1168/1168 [==============================] - 1s 452us/step - loss: 13736.4180 - val_loss: 18680.1279\n",
      "Epoch 93/150\n",
      "1168/1168 [==============================] - 1s 440us/step - loss: 13691.6356 - val_loss: 18343.7307\n",
      "Epoch 94/150\n",
      "1168/1168 [==============================] - 1s 449us/step - loss: 13669.2788 - val_loss: 18364.1460\n",
      "Epoch 95/150\n",
      "1168/1168 [==============================] - 1s 445us/step - loss: 13633.8607 - val_loss: 18457.8688\n",
      "Epoch 96/150\n",
      "1168/1168 [==============================] - 1s 442us/step - loss: 13614.5366 - val_loss: 18444.7084\n",
      "Epoch 97/150\n",
      "1168/1168 [==============================] - 1s 443us/step - loss: 13607.6096 - val_loss: 18406.5417\n",
      "Epoch 98/150\n",
      "1168/1168 [==============================] - 1s 468us/step - loss: 13585.1667 - val_loss: 18407.0524\n",
      "Epoch 99/150\n",
      "1168/1168 [==============================] - 1s 443us/step - loss: 13570.6659 - val_loss: 18373.4185\n",
      "Epoch 100/150\n",
      "1168/1168 [==============================] - 1s 446us/step - loss: 13563.4508 - val_loss: 18419.9617\n",
      "Epoch 101/150\n",
      "1168/1168 [==============================] - 1s 445us/step - loss: 13549.8326 - val_loss: 18370.3360\n",
      "Epoch 102/150\n",
      "1168/1168 [==============================] - 1s 437us/step - loss: 13538.6944 - val_loss: 18410.8956\n",
      "Epoch 103/150\n",
      "1168/1168 [==============================] - 1s 440us/step - loss: 13532.9433 - val_loss: 18371.9061\n",
      "Epoch 104/150\n",
      "1168/1168 [==============================] - 1s 446us/step - loss: 13526.7733 - val_loss: 18378.5169\n",
      "Epoch 105/150\n",
      "1168/1168 [==============================] - 1s 443us/step - loss: 13517.1660 - val_loss: 18384.6455\n",
      "Epoch 106/150\n",
      "1168/1168 [==============================] - 1s 453us/step - loss: 13511.0462 - val_loss: 18370.6308\n",
      "Epoch 107/150\n",
      "1168/1168 [==============================] - 1s 447us/step - loss: 13505.1012 - val_loss: 18369.6438\n",
      "Epoch 108/150\n",
      "1168/1168 [==============================] - 1s 434us/step - loss: 13498.8412 - val_loss: 18365.2320\n",
      "Epoch 109/150\n",
      "1168/1168 [==============================] - 1s 445us/step - loss: 13492.5273 - val_loss: 18364.9690\n",
      "Epoch 110/150\n",
      "1168/1168 [==============================] - 1s 453us/step - loss: 13486.3333 - val_loss: 18363.9989\n",
      "Epoch 111/150\n",
      "1168/1168 [==============================] - 1s 445us/step - loss: 13480.2968 - val_loss: 18362.6525\n",
      "Epoch 112/150\n",
      "1168/1168 [==============================] - 1s 457us/step - loss: 13474.8534 - val_loss: 18360.5743\n",
      "Epoch 113/150\n",
      "1168/1168 [==============================] - 1s 458us/step - loss: 13468.7073 - val_loss: 18360.2533\n",
      "Epoch 114/150\n",
      "1168/1168 [==============================] - 1s 445us/step - loss: 13463.3874 - val_loss: 18356.6932\n",
      "Epoch 115/150\n",
      "1168/1168 [==============================] - 1s 455us/step - loss: 13457.7847 - val_loss: 18356.2300\n",
      "Epoch 116/150\n",
      "1168/1168 [==============================] - 1s 441us/step - loss: 13452.2832 - val_loss: 18353.2320\n",
      "Epoch 117/150\n",
      "1168/1168 [==============================] - 1s 449us/step - loss: 13447.0682 - val_loss: 18350.7679\n",
      "Epoch 118/150\n",
      "1168/1168 [==============================] - 1s 444us/step - loss: 13441.7108 - val_loss: 18349.2517\n",
      "Epoch 119/150\n",
      "1168/1168 [==============================] - 1s 446us/step - loss: 13436.5018 - val_loss: 18347.0539\n",
      "Epoch 120/150\n",
      "1168/1168 [==============================] - 1s 436us/step - loss: 13431.0263 - val_loss: 18345.1963\n",
      "Epoch 121/150\n",
      "1168/1168 [==============================] - 1s 464us/step - loss: 13425.7493 - val_loss: 18343.9642\n",
      "Epoch 122/150\n",
      "1168/1168 [==============================] - 1s 443us/step - loss: 13420.3765 - val_loss: 18341.4305\n",
      "Epoch 123/150\n",
      "1168/1168 [==============================] - 1s 447us/step - loss: 13414.9745 - val_loss: 18338.6288\n",
      "Epoch 124/150\n",
      "1168/1168 [==============================] - 1s 450us/step - loss: 13410.0438 - val_loss: 18337.2963\n",
      "Epoch 125/150\n",
      "1168/1168 [==============================] - 1s 477us/step - loss: 13404.8203 - val_loss: 18334.2858\n",
      "Epoch 126/150\n",
      "1168/1168 [==============================] - 1s 520us/step - loss: 13399.6187 - val_loss: 18331.9797\n",
      "Epoch 127/150\n",
      "1168/1168 [==============================] - 1s 444us/step - loss: 13394.4692 - val_loss: 18329.7041\n",
      "Epoch 128/150\n",
      "1168/1168 [==============================] - 0s 421us/step - loss: 13389.1517 - val_loss: 18326.7678\n",
      "Epoch 129/150\n",
      "1168/1168 [==============================] - 0s 424us/step - loss: 13384.1565 - val_loss: 18322.9554\n",
      "Epoch 130/150\n",
      "1168/1168 [==============================] - 1s 438us/step - loss: 13379.2080 - val_loss: 18322.0616\n",
      "Epoch 131/150\n",
      "1168/1168 [==============================] - 1s 432us/step - loss: 13373.9715 - val_loss: 18318.6968\n",
      "Epoch 132/150\n",
      "1168/1168 [==============================] - 0s 425us/step - loss: 13368.9540 - val_loss: 18316.5645\n",
      "Epoch 133/150\n",
      "1168/1168 [==============================] - 1s 433us/step - loss: 13363.6744 - val_loss: 18313.4756\n",
      "Epoch 134/150\n",
      "1168/1168 [==============================] - 1s 431us/step - loss: 13358.7046 - val_loss: 18310.9670\n",
      "Epoch 135/150\n",
      "1168/1168 [==============================] - 1s 481us/step - loss: 13353.7215 - val_loss: 18308.6471\n",
      "Epoch 136/150\n",
      "1168/1168 [==============================] - 1s 480us/step - loss: 13348.3403 - val_loss: 18307.8840\n",
      "Epoch 137/150\n",
      "1168/1168 [==============================] - 1s 435us/step - loss: 13343.6600 - val_loss: 18305.1292\n",
      "Epoch 138/150\n",
      "1168/1168 [==============================] - 1s 505us/step - loss: 13338.2073 - val_loss: 18303.9906\n",
      "Epoch 139/150\n",
      "1168/1168 [==============================] - 1s 444us/step - loss: 13333.6163 - val_loss: 18300.7759\n",
      "Epoch 140/150\n",
      "1168/1168 [==============================] - 1s 488us/step - loss: 13328.5157 - val_loss: 18297.8752\n",
      "Epoch 141/150\n",
      "1168/1168 [==============================] - 1s 449us/step - loss: 13323.3018 - val_loss: 18295.1464\n",
      "Epoch 142/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 1s 432us/step - loss: 13318.4960 - val_loss: 18292.4611\n",
      "Epoch 143/150\n",
      "1168/1168 [==============================] - 1s 464us/step - loss: 13313.2915 - val_loss: 18290.7754\n",
      "Epoch 144/150\n",
      "1168/1168 [==============================] - 1s 436us/step - loss: 13308.2413 - val_loss: 18289.5898\n",
      "Epoch 145/150\n",
      "1168/1168 [==============================] - 1s 514us/step - loss: 13303.4869 - val_loss: 18286.8802\n",
      "Epoch 146/150\n",
      "1168/1168 [==============================] - 1s 533us/step - loss: 13298.4289 - val_loss: 18284.2999\n",
      "Epoch 147/150\n",
      "1168/1168 [==============================] - 1s 450us/step - loss: 13293.2485 - val_loss: 18283.0328\n",
      "Epoch 148/150\n",
      "1168/1168 [==============================] - 1s 432us/step - loss: 13288.3925 - val_loss: 18281.0671\n",
      "Epoch 149/150\n",
      "1168/1168 [==============================] - 1s 445us/step - loss: 13283.5255 - val_loss: 18277.5430\n",
      "Epoch 150/150\n",
      "1168/1168 [==============================] - 1s 430us/step - loss: 13278.4821 - val_loss: 18275.4718\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.optimizers import Adadelta, Adam\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.regularizers import l1\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "#tf.set_random_seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Model\n",
    "model = Sequential()\n",
    "#model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(200, input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(100, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(50, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(25, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "# Compile model\n",
    "model.compile(loss='mean_absolute_error', optimizer=Adadelta())\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val,y_val), epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168/1168 [==============================] - 0s 81us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13271.538099315068"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluation on the training set\n",
    "model.evaluate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAggAAAFNCAYAAABlgZchAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deZhcVZ3/8fenqjoLZA8hhARIECYIQYJAiD9HjOIAOgioqGFUojIy4r6Mozw4gALPjDKK44zioGBAEGFAhFFAUWyWGXYMhJ0QljQJZCFAOiFJL9/fH/dUUukt3Ul31e3i83qe+/Stc++5db5VSdW3zjn3XkUEZmZmZpUKtW6AmZmZ5Y8TBDMzM+vECYKZmZl14gTBzMzMOnGCYGZmZp04QTAzM7NOnCDY65qkqZJCUqkX+35c0u3VaFd6vvdJWiKpWdKB1XrePJE0R1JTrdsBffu3kleSbpA0r7/3tfrkBMEGDUnPSNooaacO5QvSB/fU2rRswPwb8LmIGBERf6l1Y6ohvY971bod1SZpvqSzt7LPdr82EfHuiLi4v/e1+uQEwQabp4ETyg8k7Q8Mr11z+l/FL9Q9gIe38RjF/mvR4PF6jRu2+Hdj1i+cINhg8wvgxIrH84BLKneQNFrSJZJWSHpW0jclFdK2oqR/k7RS0mLgb7uoe6GkZZKel3R2b750KrqfT5a0NNX/asX2gqRvSHpK0ipJV0oa16HuSZKeA26T1AwUgQckPZX2e6OkRkkvS3pY0jEVx58v6XxJ10taC7wjlf04dRU3S/pfSbtI+oGk1ZIeqxy6qGjfGkmPSHpfxbaPS7o9vXarJT0t6d0V28dJ+nmKfbWk31RsOzr18rws6f8kvamb1/DWtPpAau+HK7Z9VdLy9Lp+Yitx9/T+nynp0i7et1J6PE3Srek1+KOkH1Xun3xE0nPp39BpFccaml7bpWn5gaShla9fh3hD0l6STgY+AvxTivt/evPaKA2/SPq6pBeAn0saK+m3KfbVaX1KxXEaJf19L9/Tvuzbm9fNBhknCDbY3AmMSl+WReDDQMcPov8ARgN7Am8nSyjKXyqfAo4GDgQOBo7vUPdioBXYK+1zBPD3fWjfO4C9U71vSHpXKv8CcFxqz67AauBHHeq+HXgj8M6IGJHKDoiIN0hqAP4H+AOwM/B54DJJ0yvq/x1wDjASKH8ZfQj4JrATsAG4A7g/Pb4K+H5F/aeAt5G9dt8CLpU0qWL7ocDjqe53gQslKW37BbADsF9q33kAkt4MXAT8AzAe+C/guvIXZ6WIOKwi5hERcUV6vEtq02TgJOBHksb2EHdP7//W/BK4O7X1TOBjXezz18B04HDgdElvTOWnAbOBmcABwCyy175HEXEBcBnw3RT3e7vYp6fXZhxZb9PJZJ/pP0+PdwdeA/6zh6fv6T3ty769ed1ssIkIL14GxQI8A7yL7EP3X4CjgJuAEhDAVLJf3RuAfSvq/QPQmNZvBj5dse2IVLcETEx1h1dsPwH4c1r/OHB7N22bmo6zT0XZd4EL0/qjwOEV2yYBLel5y3X37HDMAPZK628DXgAKFdsvB85M6/OBSzrUnw/8tOLx54FHKx7vD7zcw+u9ADi2IvZFFdt2SO3bJcXSDozt4hjnA2d1KHsceHs3z7kp5vR4DtmXXKmibDkwu6u4e/H+nwlc2sX7ViL7Qm0FdqjYfml5/4p9p1RsvxuYm9afAt5Tse1I4Jnu/u10eH/nA2dv5d9/V6/NRmBYD3VmAqsrHjcCf7+197Qv+27tdfMyeBePWdlg9AvgVmAaHYYXyH7dDAGerSh7luzXJ2S/3pd02Fa2B9AALKv4EVXosP/WdDz2/hXHvkZSe8X2NrKkpKu6He0KLImIyvqVcXVX/8WK9de6eFzuqUDSicBXyL4ISdsqJ4S+UF6JiHXpNRpB9gv2pYhY3cXz7wHMk/T5irIhKZ7eWhURrRWP11W2my3j3tr735NdyeJY1+HYu3XY74WK9cq27NrF8/Ylzm2xIiLWlx9I2oGs9+YooNzLMlJSMSLauqjf3Xvale723YnevW42yHiIwQadiHiWbLLie4Bfd9i8kuyX+R4VZbsDz6f1ZWz5wbV7xfoSsl+fO0XEmLSMioj9+tC8jsdeWnHsd1ccd0xEDIuI5yv27+nWqkuB3cpj6V3EtbX6PZK0B/BT4HPA+IgYAzwEdNfdXGkJME7SmG62ndMh7h0i4vJtbWsXKuPe2vu/luzXb9kuFevLyOKo3N6XL7mlXTxv+f3f4nklVT4vbPt717HeV8mGPw6NiFFAeWiiN+/jttre181yygmCDVYnkY3Vr60sTL+SrgTOkTQyffF9hc3zFK4EviBpShrH/kZF3WVkY/zfkzRK2cTCN0h6ex/a9c+SdpC0H9m4d3ms+CepTXsASJog6dg+HPcusi+Zf5LUIGkO8F7gV304Rk92JPuyWZHa9wlgRm8qptftBuDHaZJcg6TyF9NPgU9LOlSZHSX9raSR3RzuRbK5A9ukF+//AuAwSbtLGg2cWlH3WeBe4ExJQyS9hew17q3LgW+m93Yn4PSK530A2E/STEnDyIY6KvUm7t7sM5KsZ+hlZZNgz+hD+7dJP7xullNOEGxQioinIuLebjZ/nuzLdDHZpLVfkk2Ug+wL6/dkH9j307kH4kSyLupHyCYSXkU2xt5btwCLgD8B/xYRf0jl/w5cB/xB0hqyyZaH9vagEbEROAZ4N9mv5B8DJ0bEY31oW0/HfwT4HtkkxhfJhkb+tw+H+BjZL/fHyOYIfCkd916yiaH/SfZ6LiIbz+7OmcDFys54+FCfgtis2/c/Im4iS9oeBO4Dftuh7keAtwCrgLPTvht6+bxnk31RPggsJPv3dXZ63ieAbwN/BJ5k8yTSsguBfVPcv6FrZ7L11+YHZKf9riT7N3ZjL9u+vbbndbOcUsQ290qaWaLsIk1PAw0dxsttEJN0BfBYRAz4L/F64tetPrgHwcwskXRIGlYqSDoKOBbo7he9JX7d6pPPYjAz22wXsmGn8UATcEq8Ti5zvZ38utUhDzGYmZlZJx5iMDMzs06cIJiZmVknnoOQjBkzJvba6/Vzl9m1a9ey44471roZVeN465vjrW+Od+Dcd999KyNiQlfbnCAkEydO5N57uzutvv40NjYyZ86cWjejahxvfXO89c3xDhxJz3a3zUMMZmZm1okTBDMzM+vECYKZmZl14jkIZmY2aLW0tNDU1MT69eu3vvMgMXr0aB599NF+PeawYcOYMmUKDQ0Nva7jBMHMzAatpqYmRo4cydSpU5EG8q7W1bNmzRpGjuzuhqd9FxGsWrWKpqYmpk2b1ut6HmIwM7NBa/369YwfP75ukoOBIInx48f3uZfFCYKZmQ1qTg62blteIycIZmZm1okTBDMzsyoZMWJEt9ueeeYZZsyYUcXW9MyTFJOGllfhvvk97LGV7pmtdt/kq/7EFx6FBcv65/kHvO1bO/zWn3/C8kfgoZe2uf72Pv+A1O2h/viVD8Fja7e5/vY+PwCFEgwdBUNHwqhJMGz0Vp7LzPLECUIybP1y+J8v1roZVfNGgMdq3Yrq2Q/gkVq3onr2B3io1q2oUBwK+38QDj0ZJh1Q69aY9Zuvf/3r7LHHHnzmM58B4Mwzz0QSt956K6tXr6alpYWzzz6bY489tk/HXb9+Paeccgr33nsvpVKJ73//+7zjHe/g4Ycf5hOf+AQbN26kvb2dq6++ml133ZUPfehDNDU10dbWxj//8z/z4Q9/eLtjc4KQNI+YCl+5reuNEVupvZXtOax/5113MfvQQ/vp+benbnViv/ueu5l1yKzqPn8NY7/33vs4+KA31+z5AWjdCBubYf0r8Oz/wQOXw4JLs0ThAz/byrHN+u5b//Mwjyx9tV+Pue+uozjjvft1u33u3Ll86Utf2pQgXHnlldx44418+ctfZtSoUaxcuZLZs2dzzDHH9Gmi4I9+9CMAFi5cyGOPPcYRRxzBE088wU9+8hO++MUv8pGPfISNGzfS1tbG9ddfz6677srvfvc7AF555ZXtiHgzJwhJqASjdq11M6pm/fBnYVzvz4cd7Nbt+ALsvE+tm1E1zSNXw64za92MzfY/Hg4/Ha4+CZ66udatMes3Bx54IMuXL2fp0qWsWLGCsWPHMmnSJL785S9z6623UigUeP7553nxxRfZZZdden3c22+/nc9//vMA7LPPPuyxxx488cQTvOUtb+Gcc86hqamJ97///ey9997sv//+/OM//iNf//rXOfroo3nb297WL7E5QUhWrQ/OuLb3fbIDdVpNXw6rrY0P93DcpiUbuK256z73vkTWp/b2Yec+vbq92HnJcxu547VHt+s1284mbONxt629zzy7kfs2Pr6VY/dBLxtdEAwpFRhSLLDj0BITRw1l55HD2HPCjuwwfAyM3xuW3NOXZzbrtZ5+6Q+k448/nquuuooXXniBuXPnctlll7FixQruu+8+GhoamDp1ap+vQRDd9OD93d/9HYceeii/+93vOPLII/nZz37GO9/5Tu677z6uv/56Tj31VI444ghOP/307Y7LCUKytiW49oGlvdq3Lz3u3b3JXe7b+8P2aeeudm1tbaX0wpLO+w5Qe/v0mvXhyL09bnt7O1ryTF8a0Ydd+7+9fWxCp/ctArR4UT8duw87d2PymOFc/4W3MbpYgraN239AsxyZO3cun/rUp1i5ciW33HILV155JTvvvDMNDQ38+c9/5tlnu72jcrcOO+wwLrvsMt75znfyxBNP8NxzzzF9+nQWL17MnnvuyRe+8AUWL17Mgw8+yD777MO4ceP46Ec/yogRI5g/f36/xOUEIdl9ZIEFpx9R62ZUje+vXt9qFW9be9DS1s6G1naaN7Sy/NX1PLViLV+/+kHO/t0jnDtuCLS3VL1dZgNpv/32Y82aNUyePJlJkybxkY98hPe+970cfPDBzJw5k3326fvw5mc+8xk+/elPs//++1MqlZg/fz5Dhw7liiuu4NJLL6WhoYFddtmF008/nXvuuYevfe1rFAoFGhoaOP/88/slrgFLECRdBBwNLI+IGansCmB62mUM8HJEzJQ0FXgUKPeJ3hkRn051DgLmA8OB64EvRkRIGgpcAhwErAI+HBHPpDrzgG+mY50dERcPVJxmtlmxIIqFIsMaiowe3sDkMcM5cPexLF7RzI8bn+JzB29kj/bW1MXhq99Z/Vi4cOGm9Z122ok77rijy/2am5u7PcbUqVN56KGHWLNmDcOGDeuyJ+DUU0/l1FNP3aLsyCOP5Mgjj9y2hvdgIC+UNB84qrIgIj4cETMjYiZwNfDris1PlbeVk4PkfOBkYO+0lI95ErA6IvYCzgO+AyBpHHAGcCgwCzhD0tj+Ds7Meu8Lh+/NGybsyI2PpWtRtLkXwSzvBixBiIhbgS6vTKNsttqHgMt7OoakScCoiLgjskHWS4Dj0uZjgXLPwFXA4em4RwI3RcRLEbEauIkOiYqZVdewhiLf+cCbeGl9e1bgYQZ7HVu4cCEzZ87cYjl002nn+VGrOQhvA16MiCcryqZJ+gvwKvDNiLgNmAw0VezTlMpIf5cARESrpFeA8ZXlXdQxsxo5eOo4npk0DlaQJiruWOsmmdXE/vvvz4IFC2rdjK2qVYJwAlv2HiwDdo+IVWnOwW8k7UfXZ2KV51R3t62nOluQdDLZ8AUTJkygsbGxd62vA83NzY63juU13vWvZad6/e9tt9AyZEy/HTev8Q4Ux7vZ6NGjWbNmTXUbNMDa2toGJKb169f36d9N1RMESSXg/WSTCwGIiA3AhrR+n6SngL8i+/U/paL6FKB8LmITsBvQlI45mmxIowmY06FOY1dtiYgLgAsApk+fHp7lXr8cbz5ct/j/oBneOntWv16YLK/xDhTHu9mjjz7KyJEjq9ugAbZmzZoBiWnYsGEceOCBvd6/FndzfBfwWERsGjqQNEFSMa3vSTYZcXFELAPWSJqd5hecCFybql0HzEvrxwM3p3kKvweOkDQ2TU48IpWZWa0VGrK/vhaCWe4NWIIg6XLgDmC6pCZJJ6VNc+k8OfEw4EFJD5BNOPx0RJQnOJ4C/AxYBDwF3JDKLwTGS1oEfAX4BkCqdxZwT1q+XXEsM6uhKJYThNbaNsSsH/V0C+fBbMCGGCLihG7KP95F2dVkpz12tf+9QKcbZEfEeuCD3dS5CLioD801s2pwD4LZoFGLIQYze70q9yD4NEerQxHB1772NWbMmMH+++/PFVdcAcCyZcs47LDDmDlzJjNmzOC2226jra2Nj3/845v2Pe+882rc+s58qWUzq5pIPQjtrS3+dWL974ZvwAsLt75fX+yyP7z7X3u1669//WsWLFjAAw88wMqVKznkkEM47LDD+OUvf8mRRx7JaaedRltbG+vWrWPBggU8//zzPPRQdpPAl19+uX/b3Q/8f9TMqkbF7DdJe+uGGrfErP/dfvvtnHDCCRSLRSZOnMjb3/527rnnHg455BB+/vOfc+aZZ7Jw4UJGjhzJnnvuyeLFi/n85z/PjTfeyKhRo2rd/E7cg2BmVRPFIUDWg2DW73r5S3+gdHc33MMOO4xbb72V3/3ud3zsYx/ja1/7GieeeCIPPPAAv//97/nRj37ElVdeyUUX5WvqnHsQzKxqVNw8xGBWbw477DCuuOIK2traWLFiBbfeeiuzZs3i2WefZeedd+ZTn/oUJ510Evfffz8rV66kvb2dD3zgA5x11lncf//9tW5+J+5BMLOqKc9BaPMQg9Wh973vfdxxxx0ccMABSOK73/0uu+yyCxdffDHnnnsuDQ0NjBgxgksuuYTnn3+eT3ziE7S3Z/cn+Zd/+Zcat74zJwhmVjXlHoRwD4LVkfItnCVx7rnncu65526xfd68ecybN69TvTz2GlTyEIOZVc3mIQZfB8Es75wgmFn1FIcCEG3uQTDLOycIZlY1hXIPgq+kaJZ7ThDMrHpK5TkIThCs/3R3eqFtti2vkRMEM6uaQvk6CB5isH4ybNgwVq1a5SShBxHBqlWrGDZsWJ/q+SwGM6uecg+CEwTrJ1OmTKGpqYkVK1bUuin9Zv369X3+Mt+aYcOGMWXKlD7VcYJgZlVTnoOAhxisnzQ0NDBt2rRaN6NfNTY2cuCBB9a6GR5iMLPqkYcYzAYNJwhmVjXFdLMmnCCY5Z4TBDOrmmKxwMYoEj7N0Sz3nCCYWdUUC6KFknsQzAYBJwhmVjXFgmil6ATBbBBwgmBmVVMsiI2UoN0JglneOUEws6opFUSrhxjMBgUnCGZWNYWCaA0PMZgNBk4QzKxqSh5iMBs0nCCYWdUUlE1SlHsQzHLPCYKZVU2pmM5icA+CWe45QTCzqikquw6C2ltr3RQz2wonCGZWNdmFkorIV1I0yz0nCGZWNcWCaIkSuAfBLPcGLEGQdJGk5ZIeqig7U9Lzkhak5T0V206VtEjS45KOrCg/SNLCtO2HkpTKh0q6IpXfJWlqRZ15kp5My7yBitHM+qZ8JcWC5yCY5d5A9iDMB47qovy8iJiZlusBJO0LzAX2S3V+LKmY9j8fOBnYOy3lY54ErI6IvYDzgO+kY40DzgAOBWYBZ0ga2//hmVlfle/FICcIZrk3YAlCRNwKvNTL3Y8FfhURGyLiaWARMEvSJGBURNwREQFcAhxXUefitH4VcHjqXTgSuCkiXoqI1cBNdJ2omFmVbU4QPMRglne1mIPwOUkPpiGI8i/7ycCSin2aUtnktN6xfIs6EdEKvAKM7+FYZlZjpULBQwxmg0Spys93PnAWEOnv94BPAupi3+ihnG2sswVJJ5MNXzBhwgQaGxt7aHp9aW5udrx1LK/xrm0JWijSuvG1fm1fXuMdKI63vuUl3qomCBHxYnld0k+B36aHTcBuFbtOAZam8ildlFfWaZJUAkaTDWk0AXM61Gnspj0XABcATJ8+PebMmdPVbnWpsbERx1u/8hpv84ZWrr/lPIYW1a/ty2u8A8Xx1re8xFvVIYY0p6DsfUD5DIfrgLnpzIRpZJMR746IZcAaSbPT/IITgWsr6pTPUDgeuDnNU/g9cISksWkI44hUZmY1VkyXWi6EhxjM8m7AehAkXU72S34nSU1kZxbMkTSTrMv/GeAfACLiYUlXAo8ArcBnI6ItHeoUsjMihgM3pAXgQuAXkhaR9RzMTcd6SdJZwD1pv29HRG8nS5rZACqmmzUVPEnRLPcGLEGIiBO6KL6wh/3PAc7povxeYEYX5euBD3ZzrIuAi3rdWDOrik3XQXAPglnu+UqKZlY1BUGrexDMBgUnCGZWNZJoVYlCOEEwyzsnCGZWVW2UKNAO7W1b39nMasYJgplVVVv5KuptnodglmdOEMysqtrUkK34aopmueYEwcyqql3p5Cn3IJjlmhMEM6uqNicIZoOCEwQzq6r2QjlB2FjbhphZj5wgmFlVbRpi8BwEs1xzgmBmVbVpkmKbr4VglmdOEMysqjzEYDY4OEEws6qKgocYzAYDJwhmVlUeYjAbHJwgmFlVxaYEwUMMZnnmBMHMqqrdQwxmg4ITBDOrqiiUexCcIJjlmRMEM6uqzWcxOEEwyzMnCGZWXQXfrMlsMHCCYGZV5SEGs8HBCYKZVVUUPcRgNhg4QTCzqorCkGzFQwxmueYEwcyqKnypZbNBwQmCmVVXuQfBV1I0yzUnCGZWVZvnILgHwSzPnCCYWVXJcxDMBgUnCGZWVZt7EDzEYJZnThDMrKqKxRJtFDzEYJZzA5YgSLpI0nJJD1WUnSvpMUkPSrpG0phUPlXSa5IWpOUnFXUOkrRQ0iJJP5SkVD5U0hWp/C5JUyvqzJP0ZFrmDVSMZtZ3BYlWih5iMMu5gexBmA8c1aHsJmBGRLwJeAI4tWLbUxExMy2frig/HzgZ2Dst5WOeBKyOiL2A84DvAEgaB5wBHArMAs6QNLY/AzOzbVcqiFZKHmIwy7kBSxAi4lbgpQ5lf4iI8qfCncCUno4haRIwKiLuiIgALgGOS5uPBS5O61cBh6fehSOBmyLipYhYTZaUdExUzKxGikXRQtFDDGY5V8s5CJ8Ebqh4PE3SXyTdIultqWwy0FSxT1MqK29bApCSjleA8ZXlXdQxsxorSrRQ8hCDWc6VavGkkk4DWoHLUtEyYPeIWCXpIOA3kvYD1EX1KB+mm2091enYjpPJhi+YMGECjY2NvY5hsGtubna8dSzP8b6wbAOtUWTZ88/xeD+1Mc/xDgTHW9/yEm/VE4Q0afBo4PA0bEBEbAA2pPX7JD0F/BXZr//KYYgpwNK03gTsBjRJKgGjyYY0moA5Heo0dtWWiLgAuABg+vTpMWfOnK52q0uNjY043vqV53hva36E1hdK7DZhJyb1UxvzHO9AcLz1LS/xVnWIQdJRwNeBYyJiXUX5BEnFtL4n2WTExRGxDFgjaXaaX3AicG2qdh1QPkPheODmlHD8HjhC0tg0OfGIVGZmOVAqiI0+i8Es9wasB0HS5WS/5HeS1ER2ZsGpwFDgpnS24p3pjIXDgG9LagXagE9HRHmC4ylkZ0QMJ5uzUJ63cCHwC0mLyHoO5gJExEuSzgLuSft9u+JYZlZjhUKag+DbPZvl2oAlCBFxQhfFF3az79XA1d1suxeY0UX5euCD3dS5CLio1401s6opFURLFJwgmOWcr6RoZlVVSGcxhIcYzHLNCYKZVVUpDTFEq6+DYJZnThDMrKoKBdESRQ8xmOWcEwQzq6rypZbDV1I0yzUnCGZWVcVC+VLLvheDWZ45QTCzqioWfKlls8HACYKZVVU2xOCbNZnlnRMEM6uqwqYhBvcgmOWZEwQzq6rsQkklaPccBLM8c4JgZlVVkIcYzAYDJwhmVlWlYnmSonsQzPLMCYKZVVX5UstyD4JZrjlBMLOqKhUKtFJE7kEwyzUnCGZWVcUCWQ9CewtE1Lo5ZtYNJwhmVlXFQiG7FwN4HoJZjjlBMLOqKhbIzmIAXwvBLMecIJhZVRULhewsBvDlls1yzAmCmVVVqXwlRXAPglmOOUEws6rKLpSUehCcIJjllhMEM6uq7EJJ5R4EXwvBLK+cIJhZVRWU7sUAPovBLMecIJhZVWVzEDzEYJZ3ThDMrKqKBVWc5ughBrO8coJgZlVVrDyLwac5muWWEwQzq6riFkMMnoNglldOEMysqjzEYDY4OEEws6oqShX3YvAQg1le9SpBkPQGSUPT+hxJX5A0ZmCbZmb1qOizGMwGhd72IFwNtEnaC7gQmAb8sqcKki6StFzSQxVl4yTdJOnJ9HdsxbZTJS2S9LikIyvKD5K0MG37oSSl8qGSrkjld0maWlFnXnqOJyXN62WMZlYFpaKvpGg2GPQ2QWiPiFbgfcAPIuLLwKSt1JkPHNWh7BvAnyJib+BP6TGS9gXmAvulOj+WlPogOR84Gdg7LeVjngSsjoi9gPOA76RjjQPOAA4FZgFnVCYiZlZbRYmNvlmTWe71NkFokXQCMA/4bSpr6KlCRNwKvNSh+Fjg4rR+MXBcRfmvImJDRDwNLAJmSZoEjIqIOyIigEs61Ckf6yrg8NS7cCRwU0S8FBGrgZvonKiYWY1sOUnRCYJZXvU2QfgE8BbgnIh4WtI04NJteL6JEbEMIP3dOZVPBpZU7NeUyian9Y7lW9RJvRuvAON7OJaZ5UDRd3M0GxRKvdkpIh4BvgCQuutHRsS/9mM71NXT9lC+rXW2fFLpZLLhCyZMmEBjY+NWG1ovmpubHW8dy3O8r7XGpnsxPP7owyx7pXG7j5nneAeC461veYm3VwmCpEbgmLT/AmCFpFsi4it9fL4XJU2KiGVp+GB5Km8CdqvYbwqwNJVP6aK8sk6TpBIwmmxIowmY06FOY1eNiYgLgAsApk+fHnPmzOlqt7rU2NiI461feY533cZWWv94JQDT95rG9FlztvuYeY53IDje+paXeHs7xDA6Il4F3g/8PCIOAt61Dc93Hdk8BtLfayvK56YzE6aRTUa8Ow1DrJE0O80vOLFDnfKxjgduTvMUfg8cIWls6u04IpWZWQ54iMFscOhVDwJQSr/4PwSc1psKki4n+yW/k6QmsjML/hW4UtJJwHPABwEi4mFJVwKPAK3AZwa+CkkAABr9SURBVCOiLR3qFLIzIoYDN6QFstMtfyFpEVnPwdx0rJcknQXck/b7dkR0nCxpZjVSVOV1EHwlRbO86m2C8G2yX+H/GxH3SNoTeLKnChFxQjebDu9m/3OAc7oovxeY0UX5elKC0cW2i4CLemqfmdVGdhaDT3M0y7veTlL8b+C/Kx4vBj4wUI0ys/olibbyZU58syaz3OrtpZanSLomXRnxRUlXS5qy9ZpmZp2VCsUsSfAQg1lu9XaS4s/JJgXuSnZNgf9JZWZmfVYsiHaVPMRglmO9TRAmRMTPI6I1LfOBCQPYLjOrY8WCaFODhxjMcqy3CcJKSR+VVEzLR4FVA9kwM6tfWYJQ8hCDWY71NkH4JNkpji8Ay8iuO/CJgWqUmdW3YkG0UfQQg1mO9SpBiIjnIuKYiJgQETtHxHFkF00yM+uzzUMMThDM8qq3PQhd6etlls3MgOxiSdkQgxMEs7zangShq5simZltVbEgWn0Wg1mubU+C0OUdEs3MtmbTHAT3IJjlVo9XUpS0hq4TAZHdG8HMrM9KBdHa7iEGszzrMUGIiJHVaoiZvX4Uyvdj8GmOZrm1PUMMZmbbpFROENp9oSSzvHKCYGZVV5DnIJjlnRMEM6u6UlG0+EqKZrnmBMHMqq5YEK0UPcRglmNOEMys6ooSLfgsBrM8c4JgZlVXLIjW8BCDWZ45QTCzqvMQg1n+OUEws6orFsRGStC6odZNMbNuOEEws6orFsQGGpwgmOWYEwQzq7pSQWxgCLS+VuummFk3nCCYWdUVJNbHkGySYntbrZtjZl1wgmBmVVcqitcYkj1oXV/bxphZl5wgmFnVbepBAGhxgmCWR04QzKzqSoXKHgTPQzDLIycIZlZ1xUKB16Ihe+AeBLNccoJgZlVXLMBr4R4EszyreoIgabqkBRXLq5K+JOlMSc9XlL+nos6pkhZJelzSkRXlB0lamLb9UJJS+VBJV6TyuyRNrXacZtY99yCY5V/VE4SIeDwiZkbETOAgYB1wTdp8XnlbRFwPIGlfYC6wH3AU8GNJxbT/+cDJwN5pOSqVnwSsjoi9gPOA71QhNDPrpawHISUI7kEwy6VaDzEcDjwVEc/2sM+xwK8iYkNEPA0sAmZJmgSMiog7IiKAS4DjKupcnNavAg4v9y6YWe2VCgXWtfssBrM8q3WCMBe4vOLx5yQ9KOkiSWNT2WRgScU+TalsclrvWL5FnYhoBV4Bxvd/881sWxQk9yCY5VypVk8saQhwDHBqKjofOAuI9Pd7wCeBrn75Rw/lbGVbZRtOJhuiYMKECTQ2NvY+gEGuubnZ8daxvMe79PmNvNpagAZ49MH7eXH56O06Xt7j7W+Ot77lJd6aJQjAu4H7I+JFgPJfAEk/BX6bHjYBu1XUmwIsTeVTuiivrNMkqQSMBl7q2ICIuAC4AGD69OkxZ86c7Q5qsGhsbMTx1q+8x3vna4/x8JLlALxxr6m88eA523W8vMfb3xxvfctLvLUcYjiBiuGFNKeg7H3AQ2n9OmBuOjNhGtlkxLsjYhmwRtLsNL/gRODaijrz0vrxwM1pnoKZ5UCpINaGL7Vslmc16UGQtAPwN8A/VBR/V9JMsqGAZ8rbIuJhSVcCjwCtwGcjonx3l1OA+cBw4Ia0AFwI/ELSIrKeg7kDGY+Z9U2hINa2l09z9BwEszyqSYIQEevoMGkwIj7Ww/7nAOd0UX4vMKOL8vXAB7e/pWY2ELLbPZcnKboHwSyPan0Wg5m9DhULAkSUhrkHwSynnCCYWdVlCQJQGuYeBLOccoJgZlVXSgmCexDM8ssJgplVXUHlBGG4exDMcsoJgplVXanoHgSzvHOCYGZVt7kHYah7EMxyygmCmVVdeQ5Ce9E9CGZ55QTBzKqu4ATBLPecIJhZ1W0+i8GTFM3yygmCmVVd+ToIbe5BMMstJwhmVnXFTUMMnqRolldOEMys6kruQTDLPScIZlZ15dMc29yDYJZbThDMrOrKF0pqK6Z7MUTUuEVm1pETBDOruk09CIWhWYF7EcxyxwmCmVVdqZB99LQVh2UFnodgljtOEMys6lJ+QKt7EMxyywmCmVVduQdhU4LgHgSz3HGCYGZVV0yfPC0FDzGY5ZUTBDOrumLHHgQPMZjljhMEM6u6YjqLocVDDGa55QTBzKqufKnlFrkHwSyvnCCYWdV1ShDcg2CWO04QzKzqNiUIhSFZgXsQzHLHCYKZVV2pY4LgHgSz3HGCYGZVV+5B2Eg6zdE9CGa54wTBzKpuU4KghqzAPQhmueMEwcyqbnOC4B4Es7yqSYIg6RlJCyUtkHRvKhsn6SZJT6a/Yyv2P1XSIkmPSzqyovygdJxFkn4oZSdXSxoq6YpUfpekqdWO0cy6V04Q2gIoDnEPglkO1bIH4R0RMTMiDk6PvwH8KSL2Bv6UHiNpX2AusB9wFPBjScVU53zgZGDvtByVyk8CVkfEXsB5wHeqEI+Z9VL5Qklt7QENw50gmOVQnoYYjgUuTusXA8dVlP8qIjZExNPAImCWpEnAqIi4IyICuKRDnfKxrgIOL/cumFntFYvZf8fW9oDScGh1gmCWN6UaPW8Af5AUwH9FxAXAxIhYBhARyyTtnPadDNxZUbcplbWk9Y7l5TpL0rFaJb0CjAdWVjZC0slkPRBMmDCBxsbGfgsw75qbmx1vHct7vOtbA4AnFi3itVZ4telZHt2O9uY93v7meOtbXuKtVYLw1ohYmpKAmyQ91sO+Xf3yjx7Ke6qzZUGWmFwAMH369JgzZ06Pja4njY2NON76lfd417e0wR9vZOq0PRm+bizDx49i4na0N+/x9jfHW9/yEm9NhhgiYmn6uxy4BpgFvJiGDUh/l6fdm4DdKqpPAZam8ildlG9RR1IJGA28NBCxmFnfbZqk2BbQMAxafBaDWd5UPUGQtKOkkeV14AjgIeA6YF7abR5wbVq/DpibzkyYRjYZ8e40HLFG0uw0v+DEDnXKxzoeuDnNUzCzHNg0STHKcxCcIJjlTS2GGCYC16Q5gyXglxFxo6R7gCslnQQ8B3wQICIelnQl8AjQCnw2ItrSsU4B5gPDgRvSAnAh8AtJi8h6DuZWIzAz651CQRRUPothGKx/tdZNMrMOqp4gRMRi4IAuylcBh3dT5xzgnC7K7wVmdFG+npRgmFk+FQvKEoTScGhdvvUKZlZVeTrN0cxeRzYlCA3DfB0EsxxygmBmNVFUZQ+C5yCY5Y0TBDOriWJB2YWS3INglktOEMysJooF0R7pUsvuQTDLHScIZlYTxUJh86WWW9aBz0Q2yxUnCGZWE8VCxYWSAFo31LZBZrYFJwhmVhOlQmHzhZLAN2wyyxknCGZWE4VCxYWSwJdbNssZJwhmVhOlQmHzaY7gHgSznHGCYGY1scWFksA9CGY54wTBzGpiiwslgXsQzHLGCYKZ1cQWF0oC9yCY5YwTBDOriU0XSnIPglkuOUEws5rY3IOQEgT3IJjlihMEM6uJYkG0VyYIvtyyWa44QTCzmsh6ENqhVJ6D4CEGszxxgmBmNbHpLIZNQwxOEMzyxAmCmdVEqVg+zbF8LwYnCGZ54gTBzGqi0KkHwXMQzPLECYKZ1USpoOxmTYUiFBrcg2CWM04QzKwmCgXR2hbZg4bh7kEwyxknCGZWE6XyhZIgm4fgHgSzXHGCYGY1selCSZBdbtk9CGa54gTBzGpi04WSILvccupBuOKe5/jG1Q/WsGVmBk4QzKxGtuxByOYgtLcHP/zTIn51zxJWr91Y2waavc45QTCzmiiqogehIetBuPuZl3j+5awn4a6nX6ph68zMCYKZ1cTQhgKvrm/ltY1t2STFlvVcc//z7DikyLCGAncuXlXrJpq9rlU9QZC0m6Q/S3pU0sOSvpjKz5T0vKQFaXlPRZ1TJS2S9LikIyvKD5K0MG37oSSl8qGSrkjld0maWu04zaxnx86cTPOGVi68fTE0DKd94zquX7iMo2ZM4uA9xrkHwazGatGD0Ap8NSLeCMwGPitp37TtvIiYmZbrAdK2ucB+wFHAjyUV0/7nAycDe6flqFR+ErA6IvYCzgO+U4W4zKwPDpk6jiP2nchPblnMBoawbl0zaza08oE3T+bQaeN47IVXeXmd5yGY1UrVE4SIWBYR96f1NcCjwOQeqhwL/CoiNkTE08AiYJakScCoiLgjIgK4BDiuos7Faf0q4PBy74KZ5cc/HbUPr7W08ciKjbS8toZJo4cxe8/xzH7DeCI8D8Gslmo6ByF1/R8I3JWKPifpQUkXSRqbyiYDSyqqNaWyyWm9Y/kWdSKiFXgFGD8AIZjZdthr5xHMPWQ3frN8ImPbVnHKG16iUBBvmjLa8xDMaqxUqyeWNAK4GvhSRLwq6XzgLCDS3+8BnwS6+uUfPZSzlW2VbTiZbIiCCRMm0NjY2McoBq/m5mbHW8cGU7yH7NDOt+JtfDWu4O3Lfk5j4wQA9hwFf3zwOd4+csVWjzGY4u0Pjre+5SXemiQIkhrIkoPLIuLXABHxYsX2nwK/TQ+bgN0qqk8BlqbyKV2UV9ZpklQCRgOd+ioj4gLgAoDp06fHnDlztje0QaOxsRHHW78GW7zNo5/l/nuOYc5L/80eM98AY3ZjYduTfP+PTzBz1v9jzA5Deqw/2OLdXo63vuUl3lqcxSDgQuDRiPh+Rfmkit3eBzyU1q8D5qYzE6aRTUa8OyKWAWskzU7HPBG4tqLOvLR+PHBzmqdgZjn00dl7MOdj38we3PNTgE3zEO72PASzmqjFHIS3Ah8D3tnhlMbvplMWHwTeAXwZICIeBq4EHgFuBD4bEW3pWKcAPyObuPgUcEMqvxAYL2kR8BXgG9UJzcy22Zjd4I3vhfvmw4bminkIThDMaqHqQwwRcTtdzxG4voc65wDndFF+LzCji/L1wAe3o5lmVgtv+Sw88ht44HKGzvoUB+0xlt8//ALHHzSFfXcdVevWmb2u+EqKZpYfUw6ByQfBbd+D5Y9xytv3Yt3GVo7+j9s49dcLeWblWlY2b+CV11poa/eoodlAqtlZDGZmnUhw9Hlw2QfhZ+/ir4+/iMZ/fAf//qcnueSOZ7j87uc27brr6GH8YO6BzJo2rnbtNatjThDMLF8mHQCfuhkuPwEu/zCj/+YsTj/6s3x09u7c/fRLtLS1s6G1nUvvfJa5F9zBl9/1V+xX2Nyb0N4e3PLkCi75v2d4dtU6/mriSPaZNJJZ08bxlj3H42ummfWOEwQzy5/RU+CTN8I1n4Y/nAYrHmXPvz2PPSfsvmmXubN257RrFvK9m55g5x3EtCfuYMwODTz+whqeWbWOnUcOZeZuY3jixTX8/pEXiIA3TRnNZ+bsxRH7TqRQ2Hqi0NLWzsrmDby2sY0dh5bYcWiJHRqKW9Rtbw+eWbWWDa3tTNtpR4Y1FHs4otng4QTBzPJpyI7wwYuh8V/g1u/CqsXw4V/AjjsBMGJoiR98eCZv/6sJ/KLxISLgmZXr2GX0ML5yxHSO2m8XhpSyaVbrNrZy3YKlnH/LU3z60vsY3lCkVBQCJCFlM6eLBVEsiFKhwIbWNlat3UjHE6SHFAtMHD2USaOH09rWzmMvrGHdxuzEqoJg93E7sPOoYRAQBO0B7RFEQET2OAja27Ort2Vl2fb2CCRRKohSMUtCWlqDlrZ2Ij13Q0k0r3mNYQtuzcoDCgVREBQkpGw9i2lzItOx46RTetRhh47be6rfsVemp7rquFVdrm5R7+WXX+MnT9zRczw9PEdPnUY9tb2/Yu64VVuJeeXK9Vy+5N6txrHtr2vvetGcIJhZfhUK8M7TYMJ0uPaz8B8Hwb7Hwv7Hwx5vRYUi73/zFMa9uog5c97S7WF2GFJi7qzdOf6gKdzw0Avc/9zqTV/8EZG+qKEtgra2oLU9GFISE0cNY+KoYQxvKLJ2YytrN7Syau1GXnhlPcteWU9DscCHDt6NfXcdxfCGIouWN/Pk8jWsat5IoVD+sk4JSPmLG7ooE4VC9jcIWtuCtvaoSAqyRKeltZ2WtnYKG9eyy7gdaCgWkNiUXLSXE5D0t6zjZWA6Tu/smAR13t79hNDOdaPb7T3t21UbNr9H0N6+uWU9PkcPbe9T3D28ZtsXc9dtq9S8tp1m1vX+OTrU7ylmeqjXkRMEM8u//Y+HnfaG//sPWHgV3H8xjNkD/uZbsO9xW6+flIoF3nvArrz3gF0HsLEDL7vS3sG1bkbVZPF2nwDWmyzew6ryXPpa99ucIJjZ4DDpAPjAz2DjOnjiBrjt+/DfH4fdDmXMuKOh7a+h6I80s/7i/01mNrgM2QFmfCDrOfjLpXDz2cxc8s/w+L/Bnu+APefA5DfDzvtCsaHWrTUbtJwgmNngVCjCQfNgxgd4+Np/Z78hS2HRH7MrMQKUhmVzF0bvli0jd4HhY2DYaBg5CSbuB0NH1jYGsxxzgmBmg9vQEazY+a0wZ042c+ulxbD0L9my4nFYtQie+jO0rO1QUTBuTxj/BhgyIksWho5M6yMq1iv+Dh0BDTtAoZQlKCqm9fLii9Na/XCCYGb1Q8q+8Me/IZvYWBYBG9fC+ley5eVn4YWFsOwBeGUJbHgKNjbDhuYuEok+NSBLHDYlDBXrm5KJjttToqFCOjexkB1ni8ekx1nZ/qtfhqb/7KKONr8ODOQ6Wz7fgKxvfs49lyyBlpu3v+1bPA8V6/3Z9srn3DKO3q7vsuxx+MvzPbS9p9drW9reNScIZlb/pNQrMAJGT4aJ+8L0d3e9b3vb5mRhYzNsWJMtmxKIdRDt0N6alrYOf9OyxT7lpWNZG0SqF5HVIf3NLpyQ1tu3WG9oWQProkOdtJTXoR/W6VBecd5hv6zTubyLtkxub4Ol6lTe4/ogtg/A47VuhRMEM7MtFYrZPIVho2vdkm7d39jInDlzat2Mqrlte+KNgUpuOq7Tu8SlF+t33nEHs2fPrijvePx+ansEfOvAjq/YJk4QzMysfmnrXel5s374Yhi7R62b4ds9m5mZWWdOEMzMzKwTJwhmZmbWiRMEMzMz68QJgpmZmXXiBMHMzMw6cYJgZmZmnThBMDMzs06cIJiZmVknThDMzMysE0XltZ1fxyStIRe3x6ianYCVtW5EFTne+uZ465vjHTh7RMSErjb4XgybPR4RB9e6EdUi6V7HW78cb31zvPUtL/F6iMHMzMw6cYJgZmZmnThB2OyCWjegyhxvfXO89c3x1rdcxOtJimZmZtaJexDMzMysEycIgKSjJD0uaZGkb9S6Pf1N0m6S/izpUUkPS/piKh8n6SZJT6a/Y2vd1v4iqSjpL5J+mx7XbawAksZIukrSY+l9fku9xizpy+nf8UOSLpc0rN5ilXSRpOWSHqoo6zZGSaemz6/HJR1Zm1Zvu27iPTf9e35Q0jWSxlRsG7TxdhVrxbZ/lBSSdqooq1msr/sEQVIR+BHwbmBf4ARJ+9a2Vf2uFfhqRLwRmA18NsX4DeBPEbE38Kf0uF58EXi04nE9xwrw78CNEbEPcABZ7HUXs6TJwBeAgyNiBlAE5lJ/sc4HjupQ1mWM6f/yXGC/VOfH6XNtMJlP53hvAmZExJuAJ4BToS7inU/nWJG0G/A3wHMVZTWN9XWfIACzgEURsTgiNgK/Ao6tcZv6VUQsi4j70/oasi+PyWRxXpx2uxg4rjYt7F+SpgB/C/ysorguYwWQNAo4DLgQICI2RsTL1G/MJWC4pBKwA7CUOos1Im4FXupQ3F2MxwK/iogNEfE0sIjsc23Q6CreiPhDRLSmh3cCU9L6oI63m/cW4Dzgn4DKiYE1jdUJQvZFuaTicVMqq0uSpgIHAncBEyNiGWRJBLBz7VrWr35A9h+tvaKsXmMF2BNYAfw8Dav8TNKO1GHMEfE88G9kv7KWAa9ExB+ow1i70F2Mr4fPsE8CN6T1uotX0jHA8xHxQIdNNY3VCQKoi7K6PLVD0gjgauBLEfFqrdszECQdDSyPiPtq3ZYqKgFvBs6PiAOBtQz+LvYupXH3Y4FpwK7AjpI+WttW1Vxdf4ZJOo1smPSyclEXuw3aeCXtAJwGnN7V5i7KqharE4QsI9ut4vEUsi7LuiKpgSw5uCwifp2KX5Q0KW2fBCyvVfv60VuBYyQ9QzZc9E5Jl1KfsZY1AU0RcVd6fBVZwlCPMb8LeDoiVkREC/Br4P9Rn7F21F2MdfsZJmkecDTwkdh8Tn69xfsGsoT3gfS5NQW4X9Iu1DhWJwhwD7C3pGmShpBNCLmuxm3qV5JENj79aER8v2LTdcC8tD4PuLbabetvEXFqREyJiKlk7+XNEfFR6jDWsoh4AVgiaXoqOhx4hPqM+TlgtqQd0r/rw8nm1NRjrB11F+N1wFxJQyVNA/YG7q5B+/qVpKOArwPHRMS6ik11FW9ELIyInSNiavrcagLenP5f1zbWiHjdL8B7yGbJPgWcVuv2DEB8f03WLfUgsCAt7wHGk82GfjL9HVfrtvZz3HOA36b1eo91JnBveo9/A4yt15iBbwGPAQ8BvwCG1luswOVkcyxayL4wTuopRrIu6qfI7kj77lq3v5/iXUQ2/l7+zPpJPcTbVawdtj8D7JSHWH0lRTMzM+vEQwxmZmbWiRMEMzMz68QJgpmZmXXiBMHMzMw6cYJgZmZmnThBMLPckzSnfGdOM6sOJwhmZmbWiRMEM+s3kj4q6W5JCyT9l6SipGZJ35N0v6Q/SZqQ9p0p6U5JD0q6Jt1nAUl7SfqjpAdSnTekw4+QdJWkxyRdlq6kaGYDxAmCmfULSW8EPgy8NSJmAm3AR4Adgfsj4s3ALcAZqcolwNcj4k3Aworyy4AfRcQBZPdZWJbKDwS+BOxLdgfLtw54UGavY6VaN8DM6sbhwEHAPenH/XCyGwq1A1ekfS4Ffi1pNDAmIm5J5RcD/y1pJDA5Iq4BiIj1AOl4d0dEU3q8AJgK3D7wYZm9PjlBMLP+IuDiiDh1i0Lpnzvs19P13XsaNthQsd6GP7/MBpSHGMysv/wJOF7SzgCSxknag+xz5vi0z98Bt0fEK8BqSW9L5R8DbomIV4EmScelYwyVtENVozAzwBm4mfWTiHhE0jeBP0gqkN2t7rPAWmA/SfcBr5DNU4DslsU/SQnAYuATqfxjwH9J+nY6xgerGIaZJb6bo5kNKEnNETGi1u0ws77xEIOZmZl14h4EMzMz68Q9CGZmZtaJEwQzMzPrxAmCmZmZdeIEwczMzDpxgmBmZmadOEEwMzOzTv4/Z+9m7biQku4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "#plt.gca().set_ylim(0, 1)\n",
    "plt.title('Model performance throughout training')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[153444.22 ],\n",
       "       [335497.53 ],\n",
       "       [104833.68 ],\n",
       "       [155699.5  ],\n",
       "       [331167.47 ],\n",
       "       [ 80169.625],\n",
       "       [223595.62 ],\n",
       "       [153480.75 ],\n",
       "       [ 77004.94 ],\n",
       "       [129745.195],\n",
       "       [151617.66 ],\n",
       "       [123346.625],\n",
       "       [123723.914],\n",
       "       [203029.62 ],\n",
       "       [164418.66 ],\n",
       "       [134921.   ],\n",
       "       [189217.2  ],\n",
       "       [143198.33 ],\n",
       "       [113314.59 ],\n",
       "       [214567.25 ],\n",
       "       [176109.19 ],\n",
       "       [202765.84 ],\n",
       "       [179270.86 ],\n",
       "       [128528.17 ],\n",
       "       [200684.42 ],\n",
       "       [148544.33 ],\n",
       "       [187335.22 ],\n",
       "       [101194.016],\n",
       "       [175674.11 ],\n",
       "       [204053.45 ],\n",
       "       [112049.33 ],\n",
       "       [270594.47 ],\n",
       "       [176373.1  ],\n",
       "       [105265.73 ],\n",
       "       [265678.75 ],\n",
       "       [146787.92 ],\n",
       "       [139863.7  ],\n",
       "       [206900.44 ],\n",
       "       [295497.8  ],\n",
       "       [ 91498.77 ],\n",
       "       [137126.14 ],\n",
       "       [235394.64 ],\n",
       "       [112550.71 ],\n",
       "       [361180.2  ],\n",
       "       [132479.78 ],\n",
       "       [143952.05 ],\n",
       "       [105834.36 ],\n",
       "       [136598.73 ],\n",
       "       [397213.3  ],\n",
       "       [117156.4  ],\n",
       "       [124722.484],\n",
       "       [208912.36 ],\n",
       "       [101687.53 ],\n",
       "       [216812.1  ],\n",
       "       [153253.66 ],\n",
       "       [230927.67 ],\n",
       "       [212372.66 ],\n",
       "       [166892.64 ],\n",
       "       [127534.58 ],\n",
       "       [106008.85 ],\n",
       "       [ 67828.31 ],\n",
       "       [151831.67 ],\n",
       "       [309013.06 ],\n",
       "       [268202.2  ],\n",
       "       [313146.88 ],\n",
       "       [187804.53 ],\n",
       "       [106154.1  ],\n",
       "       [322809.72 ],\n",
       "       [116783.45 ],\n",
       "       [169794.94 ],\n",
       "       [110444.914],\n",
       "       [123203.195],\n",
       "       [110848.03 ],\n",
       "       [ 76308.31 ],\n",
       "       [457290.   ],\n",
       "       [193156.86 ],\n",
       "       [294608.4  ],\n",
       "       [325346.78 ],\n",
       "       [136752.52 ],\n",
       "       [111205.34 ],\n",
       "       [107092.62 ],\n",
       "       [ 82109.89 ],\n",
       "       [100301.73 ],\n",
       "       [ 92939.516],\n",
       "       [159618.27 ],\n",
       "       [117176.32 ],\n",
       "       [257910.8  ],\n",
       "       [205820.83 ],\n",
       "       [138992.38 ],\n",
       "       [188791.67 ],\n",
       "       [123773.734],\n",
       "       [165448.4  ],\n",
       "       [134296.45 ],\n",
       "       [267278.34 ],\n",
       "       [ 90096.16 ],\n",
       "       [178316.44 ],\n",
       "       [174522.62 ],\n",
       "       [176613.08 ],\n",
       "       [197168.2  ],\n",
       "       [285927.38 ],\n",
       "       [151952.98 ],\n",
       "       [210285.75 ],\n",
       "       [283499.28 ],\n",
       "       [141652.08 ],\n",
       "       [170618.03 ],\n",
       "       [160002.78 ],\n",
       "       [151172.39 ],\n",
       "       [269387.8  ],\n",
       "       [148978.22 ],\n",
       "       [201387.31 ],\n",
       "       [ 70037.836],\n",
       "       [109346.61 ],\n",
       "       [151557.3  ],\n",
       "       [125729.58 ],\n",
       "       [200488.58 ],\n",
       "       [105049.91 ],\n",
       "       [ 99590.36 ],\n",
       "       [115153.95 ],\n",
       "       [104917.89 ],\n",
       "       [277771.12 ],\n",
       "       [152254.42 ],\n",
       "       [138911.4  ],\n",
       "       [173455.52 ],\n",
       "       [190202.5  ],\n",
       "       [198711.11 ],\n",
       "       [137791.42 ],\n",
       "       [230573.22 ],\n",
       "       [101772.69 ],\n",
       "       [142124.33 ],\n",
       "       [191293.92 ],\n",
       "       [185379.69 ],\n",
       "       [348544.28 ],\n",
       "       [185100.31 ],\n",
       "       [121403.555],\n",
       "       [ 62465.652],\n",
       "       [372094.53 ],\n",
       "       [348632.22 ],\n",
       "       [129685.   ],\n",
       "       [236190.92 ],\n",
       "       [514769.84 ],\n",
       "       [350255.97 ],\n",
       "       [129000.55 ],\n",
       "       [177849.69 ],\n",
       "       [178087.75 ],\n",
       "       [115796.02 ],\n",
       "       [127217.97 ],\n",
       "       [198582.47 ],\n",
       "       [188223.33 ],\n",
       "       [126372.43 ],\n",
       "       [ 73988.35 ],\n",
       "       [ 95348.67 ],\n",
       "       [135342.66 ],\n",
       "       [206756.67 ],\n",
       "       [148611.4  ],\n",
       "       [ 73627.484],\n",
       "       [105010.29 ],\n",
       "       [102327.17 ],\n",
       "       [136734.23 ],\n",
       "       [ 93241.58 ],\n",
       "       [134409.1  ],\n",
       "       [198922.72 ],\n",
       "       [113233.67 ],\n",
       "       [304234.28 ],\n",
       "       [147877.86 ],\n",
       "       [108261.51 ],\n",
       "       [ 90385.02 ],\n",
       "       [241996.3  ],\n",
       "       [355090.78 ],\n",
       "       [435912.03 ],\n",
       "       [211760.62 ],\n",
       "       [365148.72 ],\n",
       "       [ 93266.08 ],\n",
       "       [106633.516],\n",
       "       [113550.36 ],\n",
       "       [300176.7  ],\n",
       "       [109473.48 ],\n",
       "       [112666.17 ],\n",
       "       [221429.27 ],\n",
       "       [127605.54 ],\n",
       "       [162570.42 ],\n",
       "       [180896.53 ],\n",
       "       [105098.02 ],\n",
       "       [120328.1  ],\n",
       "       [151941.52 ],\n",
       "       [249915.75 ],\n",
       "       [125225.22 ],\n",
       "       [284557.97 ],\n",
       "       [231618.34 ],\n",
       "       [194408.03 ],\n",
       "       [ 82997.13 ],\n",
       "       [112548.58 ],\n",
       "       [ 89880.98 ],\n",
       "       [151058.2  ],\n",
       "       [158803.39 ],\n",
       "       [187023.2  ],\n",
       "       [202574.22 ],\n",
       "       [197111.34 ],\n",
       "       [ 89062.4  ],\n",
       "       [193754.6  ],\n",
       "       [140537.62 ],\n",
       "       [236956.22 ],\n",
       "       [207852.7  ],\n",
       "       [110382.2  ],\n",
       "       [297505.03 ],\n",
       "       [194373.45 ],\n",
       "       [111039.81 ],\n",
       "       [235672.86 ],\n",
       "       [130719.69 ],\n",
       "       [148378.61 ],\n",
       "       [111123.46 ],\n",
       "       [222929.36 ],\n",
       "       [146913.58 ],\n",
       "       [110182.38 ],\n",
       "       [165467.75 ],\n",
       "       [218974.   ],\n",
       "       [253330.98 ],\n",
       "       [220741.03 ],\n",
       "       [145819.36 ],\n",
       "       [114542.68 ],\n",
       "       [110537.95 ],\n",
       "       [138124.06 ],\n",
       "       [212855.84 ],\n",
       "       [217673.38 ],\n",
       "       [ 91646.91 ],\n",
       "       [220407.6  ],\n",
       "       [133096.28 ],\n",
       "       [ 88635.32 ],\n",
       "       [ 96303.17 ],\n",
       "       [167114.12 ],\n",
       "       [106093.75 ],\n",
       "       [ 98575.98 ],\n",
       "       [171674.52 ],\n",
       "       [115277.31 ],\n",
       "       [113714.28 ],\n",
       "       [235575.92 ],\n",
       "       [145845.92 ],\n",
       "       [197164.48 ],\n",
       "       [157292.27 ],\n",
       "       [247759.7  ],\n",
       "       [119491.63 ],\n",
       "       [106109.52 ],\n",
       "       [227707.11 ],\n",
       "       [200657.9  ],\n",
       "       [427726.75 ],\n",
       "       [206865.53 ],\n",
       "       [131483.61 ],\n",
       "       [158699.27 ],\n",
       "       [176130.06 ],\n",
       "       [125001.22 ],\n",
       "       [ 87071.64 ],\n",
       "       [193212.12 ],\n",
       "       [153855.16 ],\n",
       "       [129153.72 ],\n",
       "       [ 97392.66 ],\n",
       "       [150020.73 ],\n",
       "       [138438.84 ],\n",
       "       [115157.65 ],\n",
       "       [105002.92 ],\n",
       "       [173793.12 ],\n",
       "       [250341.42 ],\n",
       "       [286658.38 ],\n",
       "       [174507.33 ],\n",
       "       [130891.21 ],\n",
       "       [234760.33 ],\n",
       "       [355838.34 ],\n",
       "       [241795.17 ],\n",
       "       [185183.1  ],\n",
       "       [130861.18 ],\n",
       "       [ 90254.33 ],\n",
       "       [166895.66 ],\n",
       "       [400165.28 ],\n",
       "       [202474.77 ],\n",
       "       [240447.52 ],\n",
       "       [ 76824.484],\n",
       "       [ 83296.46 ],\n",
       "       [138278.48 ],\n",
       "       [124789.23 ],\n",
       "       [304124.6  ],\n",
       "       [249358.23 ],\n",
       "       [126131.12 ],\n",
       "       [198515.17 ],\n",
       "       [ 92147.63 ],\n",
       "       [197477.19 ],\n",
       "       [ 91860.95 ],\n",
       "       [308689.72 ],\n",
       "       [160523.23 ],\n",
       "       [206537.72 ],\n",
       "       [137869.   ],\n",
       "       [272702.06 ],\n",
       "       [194787.97 ],\n",
       "       [ 93594.85 ],\n",
       "       [118010.43 ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_predict = model.predict(X_val)\n",
    "y_val_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute MAE, RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18275.47175995291"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mean_absolute_error(y_val, y_val_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29925.44840572048\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "root_mean_squared_error = sqrt(mean_squared_error(y_val, y_val_predict))\n",
    "print(root_mean_squared_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of the test and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_CWD</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_Con</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1461</td>\n",
       "      <td>20</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1961</td>\n",
       "      <td>1961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1462</td>\n",
       "      <td>20</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1958</td>\n",
       "      <td>1958</td>\n",
       "      <td>108.0</td>\n",
       "      <td>923.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1463</td>\n",
       "      <td>60</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1997</td>\n",
       "      <td>1998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>791.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1464</td>\n",
       "      <td>60</td>\n",
       "      <td>78.0</td>\n",
       "      <td>9978</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1998</td>\n",
       "      <td>1998</td>\n",
       "      <td>20.0</td>\n",
       "      <td>602.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1465</td>\n",
       "      <td>120</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5005</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>263.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1454</td>\n",
       "      <td>2915</td>\n",
       "      <td>160</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1936</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1455</td>\n",
       "      <td>2916</td>\n",
       "      <td>160</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1894</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1970</td>\n",
       "      <td>1970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1456</td>\n",
       "      <td>2917</td>\n",
       "      <td>20</td>\n",
       "      <td>160.0</td>\n",
       "      <td>20000</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1960</td>\n",
       "      <td>1996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1224.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1457</td>\n",
       "      <td>2918</td>\n",
       "      <td>85</td>\n",
       "      <td>62.0</td>\n",
       "      <td>10441</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1992</td>\n",
       "      <td>1992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>337.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1458</td>\n",
       "      <td>2919</td>\n",
       "      <td>60</td>\n",
       "      <td>74.0</td>\n",
       "      <td>9627</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1993</td>\n",
       "      <td>1994</td>\n",
       "      <td>94.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows Ã— 304 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  \\\n",
       "0     1461          20         80.0    11622            5            6   \n",
       "1     1462          20         81.0    14267            6            6   \n",
       "2     1463          60         74.0    13830            5            5   \n",
       "3     1464          60         78.0     9978            6            6   \n",
       "4     1465         120         43.0     5005            8            5   \n",
       "...    ...         ...          ...      ...          ...          ...   \n",
       "1454  2915         160         21.0     1936            4            7   \n",
       "1455  2916         160         21.0     1894            4            5   \n",
       "1456  2917          20        160.0    20000            5            7   \n",
       "1457  2918          85         62.0    10441            5            5   \n",
       "1458  2919          60         74.0     9627            7            5   \n",
       "\n",
       "      YearBuilt  YearRemodAdd  MasVnrArea  BsmtFinSF1  ...  SaleType_CWD  \\\n",
       "0          1961          1961         0.0       468.0  ...           0.0   \n",
       "1          1958          1958       108.0       923.0  ...           0.0   \n",
       "2          1997          1998         0.0       791.0  ...           0.0   \n",
       "3          1998          1998        20.0       602.0  ...           0.0   \n",
       "4          1992          1992         0.0       263.0  ...           0.0   \n",
       "...         ...           ...         ...         ...  ...           ...   \n",
       "1454       1970          1970         0.0         0.0  ...           0.0   \n",
       "1455       1970          1970         0.0       252.0  ...           0.0   \n",
       "1456       1960          1996         0.0      1224.0  ...           0.0   \n",
       "1457       1992          1992         0.0       337.0  ...           0.0   \n",
       "1458       1993          1994        94.0       758.0  ...           0.0   \n",
       "\n",
       "      SaleType_ConLw  SaleType_Con  SaleType_Oth  SaleCondition_Normal  \\\n",
       "0                0.0           0.0           0.0                     1   \n",
       "1                0.0           0.0           0.0                     1   \n",
       "2                0.0           0.0           0.0                     1   \n",
       "3                0.0           0.0           0.0                     1   \n",
       "4                0.0           0.0           0.0                     1   \n",
       "...              ...           ...           ...                   ...   \n",
       "1454             0.0           0.0           0.0                     1   \n",
       "1455             0.0           0.0           0.0                     0   \n",
       "1456             0.0           0.0           0.0                     0   \n",
       "1457             0.0           0.0           0.0                     1   \n",
       "1458             0.0           0.0           0.0                     1   \n",
       "\n",
       "      SaleCondition_Abnorml  SaleCondition_Partial  SaleCondition_AdjLand  \\\n",
       "0                         0                      0                      0   \n",
       "1                         0                      0                      0   \n",
       "2                         0                      0                      0   \n",
       "3                         0                      0                      0   \n",
       "4                         0                      0                      0   \n",
       "...                     ...                    ...                    ...   \n",
       "1454                      0                      0                      0   \n",
       "1455                      1                      0                      0   \n",
       "1456                      1                      0                      0   \n",
       "1457                      0                      0                      0   \n",
       "1458                      0                      0                      0   \n",
       "\n",
       "      SaleCondition_Alloca  SaleCondition_Family  \n",
       "0                        0                     0  \n",
       "1                        0                     0  \n",
       "2                        0                     0  \n",
       "3                        0                     0  \n",
       "4                        0                     0  \n",
       "...                    ...                   ...  \n",
       "1454                     0                     0  \n",
       "1455                     0                     0  \n",
       "1456                     0                     0  \n",
       "1457                     0                     0  \n",
       "1458                     0                     0  \n",
       "\n",
       "[1459 rows x 304 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('test_ohe.csv')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/tf2/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/opt/conda/envs/tf2/lib/python3.7/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "id_col = test['Id'].values.tolist()\n",
    "scale = StandardScaler()\n",
    "X_test = test[Features]\n",
    "X_test = scale.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['Id'] = id_col\n",
    "submission['SalePrice'] = prediction\n",
    "submission.to_csv('prediction_keras_allfeat_ohe.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Kaggle\n",
    "<center> Optimizer Adam (lr=0,01): </center> \n",
    "Dense 128, 1 : 0.41727  \n",
    "\n",
    "Dense 200, 100, 50, 25, 1 : 0.24114\n",
    "<center> Optimizer Adadelta: </center>  \n",
    "Dense 200, 100, 50, 25, 1 : 0.15647  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
